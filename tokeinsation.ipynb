{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postional Embedding\n",
    "# since we feed all tokens of sequence to transformer at once, we need to add some positional information to the tokens\n",
    "# positional embedding maybe explicilty learned like token embedding or it can be hardcoded\n",
    "# we will use hardcoded positional embedding as both generate similar results and harcoding reduces the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PE(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "# PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "\n",
    "# rewritten as\n",
    "# PE(pos, i) = sin(pos/10000^(i/d_model))      # for i:even\n",
    "# PE(pos, i) = cos(pos/10000^(i-1/d_model))    # for i:odd\n",
    "\n",
    "# pos: position of token in sequence\n",
    "# i: dimension of positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 4                  # sequence length  \n",
    "d_model = 16            # model dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14.]),\n",
       " tensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_i = torch.arange(0, d_model, 2).float()\n",
    "odd_i = torch.arange(1, d_model, 2).float()\n",
    "even_i, odd_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 3.1623e+00, 1.0000e+01, 3.1623e+01, 1.0000e+02, 3.1623e+02,\n",
       "        1.0000e+03, 3.1623e+03])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_den = torch.pow(10000,(even_i/d_model))\n",
    "even_den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 3.1623e+00, 1.0000e+01, 3.1623e+01, 1.0000e+02, 3.1623e+02,\n",
       "        1.0000e+03, 3.1623e+03])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_den = torch.pow(10000,((odd_i-1)/d_model))\n",
    "odd_den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both den are same\n",
    "denominator = even_den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = torch.arange(0, T).float().unsqueeze(1)\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "        [ 0.5403,  0.9504,  0.9950,  0.9995,  0.9999,  1.0000,  1.0000,  1.0000],\n",
       "        [-0.4161,  0.8066,  0.9801,  0.9980,  0.9998,  1.0000,  1.0000,  1.0000],\n",
       "        [-0.9900,  0.5828,  0.9553,  0.9955,  0.9996,  1.0000,  1.0000,  1.0000]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_pos = torch.cos(pos/denominator)\n",
    "odd_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [8.4147e-01, 3.1098e-01, 9.9833e-02, 3.1618e-02, 9.9998e-03, 3.1623e-03,\n",
       "         1.0000e-03, 3.1623e-04],\n",
       "        [9.0930e-01, 5.9113e-01, 1.9867e-01, 6.3203e-02, 1.9999e-02, 6.3245e-03,\n",
       "         2.0000e-03, 6.3246e-04],\n",
       "        [1.4112e-01, 8.1265e-01, 2.9552e-01, 9.4726e-02, 2.9995e-02, 9.4867e-03,\n",
       "         3.0000e-03, 9.4868e-04]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_pos = torch.sin(pos/denominator)\n",
    "even_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  3.1098e-01,  9.5042e-01,  9.9833e-02,\n",
       "          9.9500e-01,  3.1618e-02,  9.9950e-01,  9.9998e-03,  9.9995e-01,\n",
       "          3.1623e-03,  9.9999e-01,  1.0000e-03,  1.0000e+00,  3.1623e-04,\n",
       "          1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  5.9113e-01,  8.0658e-01,  1.9867e-01,\n",
       "          9.8007e-01,  6.3203e-02,  9.9800e-01,  1.9999e-02,  9.9980e-01,\n",
       "          6.3245e-03,  9.9998e-01,  2.0000e-03,  1.0000e+00,  6.3246e-04,\n",
       "          1.0000e+00],\n",
       "        [ 1.4112e-01, -9.8999e-01,  8.1265e-01,  5.8275e-01,  2.9552e-01,\n",
       "          9.5534e-01,  9.4726e-02,  9.9550e-01,  2.9995e-02,  9.9955e-01,\n",
       "          9.4867e-03,  9.9995e-01,  3.0000e-03,  1.0000e+00,  9.4868e-04,\n",
       "          1.0000e+00]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to interlevae even and odd pos\n",
    "# 0 1 2 3 ,.. \n",
    "# even pos: 0,2,4,... odd pos:1,3,5,..\n",
    "stacked = torch.stack([even_pos, odd_pos], dim=2)        # it will stack even and odd pos along 2nd dimension (interleaving)\n",
    "PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self,seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.T = seq_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        odd_i = torch.arange(1, self.d_model, 2).float() \n",
    "        denominator = torch.pow(10000,(even_i/self.d_model))\n",
    "        pos = torch.arange(0, self.T).float().unsqueeze(1)\n",
    "        odd_pos = torch.cos(pos/denominator)\n",
    "        even_pos = torch.sin(pos/denominator)\n",
    "        stacked = torch.stack([even_pos, odd_pos], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "\n",
    "        return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  3.1098e-01,  9.5042e-01,  9.9833e-02,\n",
       "          9.9500e-01,  3.1618e-02,  9.9950e-01,  9.9998e-03,  9.9995e-01,\n",
       "          3.1623e-03,  9.9999e-01,  1.0000e-03,  1.0000e+00,  3.1623e-04,\n",
       "          1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  5.9113e-01,  8.0658e-01,  1.9867e-01,\n",
       "          9.8007e-01,  6.3203e-02,  9.9800e-01,  1.9999e-02,  9.9980e-01,\n",
       "          6.3245e-03,  9.9998e-01,  2.0000e-03,  1.0000e+00,  6.3246e-04,\n",
       "          1.0000e+00],\n",
       "        [ 1.4112e-01, -9.8999e-01,  8.1265e-01,  5.8275e-01,  2.9552e-01,\n",
       "          9.5534e-01,  9.4726e-02,  9.9550e-01,  2.9995e-02,  9.9955e-01,\n",
       "          9.4867e-03,  9.9995e-01,  3.0000e-03,  1.0000e+00,  9.4868e-04,\n",
       "          1.0000e+00]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEmbedding(4,16)\n",
    "pe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token embeding is a simple embedding layer\n",
    "# vocab size x embedding size table\n",
    "# given a token, it will return its embedding (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokensiation\n",
    "# convert text to tokens\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    '''\n",
    "        It tokenise the sentence and then add token, positional emebedding to it\n",
    "    '''\n",
    "\n",
    "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN, dropout_ratio = 0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(language_to_index)             # language_to_index is a dictionary\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.position_encoder = PositionalEmbedding(max_sequence_length, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.START_TOKEN = START_TOKEN\n",
    "        self.END_TOKEN = END_TOKEN\n",
    "        self.PADDING_TOKEN = PADDING_TOKEN\n",
    "    \n",
    "    def batch_tokenize(self, batch, start_token=True, end_token=True):\n",
    "\n",
    "        def tokenize(sentence, start_token=True, end_token=True):\n",
    "            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
    "            # start token\n",
    "            if start_token:\n",
    "                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
    "            # end token\n",
    "            if end_token:\n",
    "                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
    "            # padding token\n",
    "            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
    "                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
    "            return torch.tensor(sentence_word_indicies)\n",
    "\n",
    "        tokenized = []\n",
    "        for sentence_num in range(len(batch)):\n",
    "           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
    "        tokenized = torch.stack(tokenized)\n",
    "        return tokenized.to(device)\n",
    "    \n",
    "    def forward(self, x,start_token = True, end_token=True): \n",
    "        # x: batch of sentences\n",
    "        x = self.batch_tokenize(x ,start_token, end_token)\n",
    "        print(x)\n",
    "        x = self.embedding(x)\n",
    "        pos = self.position_encoder().to(device)\n",
    "        x = self.dropout(x + pos)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " ' ': 0,\n",
       " '<sos>': 26,\n",
       " '<eos>': 27,\n",
       " '<pad>': 28}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lan_dict = {chr(i):i-96 for i in range(97,123)}\n",
    "lan_dict[' '] = 0\n",
    "lan_dict['<sos>'] = 26\n",
    "lan_dict['<eos>'] = 27\n",
    "lan_dict['<pad>'] = 28\n",
    "lan_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [\"hello i am billy russo\",\n",
    "        \"hello i am frank castle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = TransformerEmbedding(30,12,lan_dict,'<sos>','<eos>','<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[26,  8,  5, 12, 12, 15,  0,  9,  0,  1, 13,  0,  2,  9, 12, 12, 25,  0,\n",
      "         18, 21, 19, 19, 15, 27, 28, 28, 28, 28, 28, 28],\n",
      "        [26,  8,  5, 12, 12, 15,  0,  9,  0,  1, 13,  0,  6, 18,  1, 14, 11,  0,\n",
      "          3,  1, 19, 20, 12,  5, 27, 28, 28, 28, 28, 28]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5624,  3.9419, -1.6502,  1.8174,  0.0120,  1.8498,  0.6854,\n",
       "           2.4412, -0.9975,  1.7850, -0.4718,  0.0000],\n",
       "         [ 0.0000,  1.1149,  0.0000,  0.4208,  0.3114,  1.6620, -0.5500,\n",
       "           1.5107,  1.7444,  2.3122,  0.1903,  2.0085],\n",
       "         [ 1.0338, -1.6904,  0.3723, -0.3224,  0.8514,  2.2137, -0.0000,\n",
       "           1.4107, -0.8872,  1.6738,  0.1138,  0.7569],\n",
       "         [ 1.0515, -0.3677, -1.1151,  1.5340, -0.1522, -0.1699, -1.6942,\n",
       "           0.0000, -1.4106,  1.5418, -0.8397,  0.9029],\n",
       "         [ 0.0538,  0.0060, -0.9409,  1.3704, -0.1014, -0.1782, -1.6831,\n",
       "           0.8122, -1.4082,  1.5418, -0.8392,  0.9029],\n",
       "         [-1.8316,  0.3362,  1.0790, -0.6102,  0.3570,  0.0000,  1.0911,\n",
       "           0.0000,  1.3630, -0.4152, -1.0151,  2.4800],\n",
       "         [-0.1023, -0.1490,  1.1582, -0.2568,  0.5652, -0.6881,  1.0307,\n",
       "           0.4207, -0.1828,  0.0000,  0.6767,  3.2753],\n",
       "         [ 0.0280,  1.9001,  1.2704,  0.3626, -0.2348,  0.0000,  0.0000,\n",
       "           3.4706, -0.0680,  1.1773,  1.1671, -0.0000],\n",
       "         [ 1.3074, -1.3775,  1.1880, -0.7309,  0.6629, -0.7210,  1.0529,\n",
       "           0.4192, -0.1780,  0.5627,  0.6777,  3.2753],\n",
       "         [-1.2973,  0.5572, -0.0173, -2.9553,  0.0000,  1.4908,  1.9820,\n",
       "           2.1740, -1.5487, -0.4790,  0.0000,  0.0000],\n",
       "         [-2.5932,  1.2266,  2.3327, -0.5464,  0.6749,  0.4647, -0.1210,\n",
       "           2.3199,  1.1592,  1.8808, -0.9852,  0.6968],\n",
       "         [-0.9030, -1.2109,  0.8647, -0.0000,  0.8027, -0.7870,  1.0861,\n",
       "           0.4160, -0.1708,  0.5626,  0.6793,  3.2753],\n",
       "         [-0.0000,  0.5669,  0.0000, -0.5179,  1.1337,  0.0000, -0.2077,\n",
       "           2.2800,  0.0740,  0.5886,  0.7137,  0.7857],\n",
       "         [-0.2351,  2.0707,  0.5328, -0.7543,  0.0410,  0.4634,  0.2584,\n",
       "           3.4639, -0.0000,  0.0000,  1.1702, -0.8618],\n",
       "         [ 0.0000,  0.0000, -1.6453, -0.4554,  0.3658, -0.3856, -1.5725,\n",
       "           0.8022, -1.3843,  1.5413, -0.8340,  0.9029],\n",
       "         [ 1.6173, -0.1118, -1.8842, -0.4597,  0.4061, -0.4177, -0.0000,\n",
       "           0.8006, -1.3819,  1.5412, -0.8335,  0.9029],\n",
       "         [ 0.5439, -0.0000, -1.4642, -3.9560,  1.0670, -0.9684, -0.3945,\n",
       "          -0.3533,  1.6861, -0.0340,  0.6742,  1.8148],\n",
       "         [-0.8601, -1.5215, -0.4632, -1.5256,  1.0483, -0.9736,  1.1521,\n",
       "           0.4067, -0.1565,  0.5622,  0.6824,  3.2753],\n",
       "         [ 0.1413,  0.0545, -1.4993, -2.2586,  2.3466,  3.2466, -1.0918,\n",
       "           2.8465,  0.0000,  1.9562,  0.0332,  2.1782],\n",
       "         [-1.3133, -0.5642, -1.3325,  0.3841,  0.5751,  0.0000,  0.0133,\n",
       "           0.7360, -0.0000, -0.9484,  1.7919,  1.0904],\n",
       "         [ 0.8427,  0.6230, -1.7769,  0.2384,  0.6843,  0.6268,  2.0736,\n",
       "           1.2419,  0.0678,  0.0000,  0.7810,  0.7619],\n",
       "         [ 0.7580, -0.0000, -1.8465,  0.4669,  0.7142,  0.5848,  2.0845,\n",
       "           1.2396,  0.0702,  1.5013,  0.7815,  0.7619],\n",
       "         [-0.7760, -0.0000, -1.0101, -1.1062,  1.0489,  0.3123,  1.2781,\n",
       "           1.5490,  1.4037, -0.4164, -1.0063,  2.4800],\n",
       "         [-1.4455, -0.1700, -0.3513,  1.1466,  0.3516,  2.2493,  1.0024,\n",
       "          -0.0887,  0.7777,  0.8236, -0.8328,  0.4751],\n",
       "         [-0.0000,  0.4484, -2.6058,  0.3405,  2.7072,  2.0629, -2.2498,\n",
       "          -0.1321,  0.0100,  1.1961,  0.3776,  1.3522],\n",
       "         [-1.3722,  1.0784, -2.4777,  0.5422,  2.7289,  2.0161, -2.2390,\n",
       "          -0.1348,  0.0124,  0.0000,  0.3781,  1.3522],\n",
       "         [-0.3779,  0.6959, -2.3094,  0.7118,  2.7484,  1.9683, -0.0000,\n",
       "          -0.1376,  0.0148,  1.1959,  0.3786,  1.3522],\n",
       "         [-0.1625, -0.3475, -0.0000,  0.8415,  2.7656,  1.9197, -2.2176,\n",
       "          -0.1405,  0.0172,  0.0000,  0.3791,  1.3522],\n",
       "         [-0.9242, -1.0925, -1.8851,  0.9254,  2.7806,  1.8704, -2.2069,\n",
       "          -0.0000,  0.0196,  1.1956,  0.3796,  1.3522],\n",
       "         [-1.9626, -0.8541, -1.6486,  0.9594,  2.7932,  1.8204, -2.1962,\n",
       "          -0.0000,  0.0219,  1.1955,  0.3801,  0.0000]],\n",
       "\n",
       "        [[-0.5624,  3.9419, -1.6502,  1.8174,  0.0120,  1.8498,  0.0000,\n",
       "           2.4412, -0.9975,  1.7850, -0.0000,  1.1473],\n",
       "         [ 0.0000,  1.1149,  1.5244,  0.4208,  0.0000,  1.6620, -0.5500,\n",
       "           1.5107,  1.7444,  2.3122,  0.1903,  2.0085],\n",
       "         [ 1.0338, -1.6904,  0.3723, -0.3224,  0.8514,  2.2137, -1.2688,\n",
       "           1.4107, -0.8872,  1.6738,  0.1138,  0.7569],\n",
       "         [ 1.0515, -0.3677, -1.1151,  0.0000, -0.0000, -0.1699, -1.6942,\n",
       "           0.8126, -1.4106,  1.5418, -0.8397,  0.0000],\n",
       "         [ 0.0000,  0.0060, -0.9409,  1.3704, -0.0000, -0.1782, -1.6831,\n",
       "           0.8122, -1.4082,  1.5418, -0.8392,  0.9029],\n",
       "         [-1.8316,  0.3362,  1.0790, -0.6102,  0.3570,  0.8132,  1.0911,\n",
       "           1.5744,  1.3630, -0.4152, -0.0000,  2.4800],\n",
       "         [-0.1023, -0.1490,  1.1582, -0.2568,  0.5652, -0.6881,  0.0000,\n",
       "           0.4207, -0.1828,  0.5628,  0.6767,  3.2753],\n",
       "         [ 0.0280,  1.9001,  0.0000,  0.3626, -0.2348,  0.6015,  0.1921,\n",
       "           3.4706, -0.0680,  1.1773,  1.1671, -0.8617],\n",
       "         [ 1.3074, -1.3775,  1.1880, -0.7309,  0.6629, -0.7210,  1.0529,\n",
       "           0.4192, -0.1780,  0.5627,  0.6777,  3.2753],\n",
       "         [-0.0000,  0.5572, -0.0173, -2.9553,  1.4538,  1.4908,  1.9820,\n",
       "           2.1740, -1.5487, -0.4790,  0.5308,  2.2969],\n",
       "         [-2.5932,  1.2266,  2.3327, -0.5464,  0.6749,  0.0000, -0.1210,\n",
       "           0.0000,  1.1592,  1.8808, -0.9852,  0.6968],\n",
       "         [-0.9030, -0.0000,  0.8647, -1.3582,  0.8027, -0.7870,  1.0861,\n",
       "           0.4160, -0.1708,  0.5626,  0.0000,  3.2753],\n",
       "         [-0.6579,  1.9115, -0.0552, -0.9463,  0.5352,  2.4850, -0.7748,\n",
       "          -0.3190, -1.1174, -0.4449, -0.3491,  1.3789],\n",
       "         [ 1.4426,  0.3291, -0.3816, -2.4826,  2.1530,  3.4162, -1.1466,\n",
       "           2.8551,  1.3902,  1.9566,  0.0306,  2.1782],\n",
       "         [-0.6545,  1.7215, -0.9150, -3.6578,  1.6753,  1.3599,  2.0372,\n",
       "           2.1676, -1.5368, -0.4793,  0.5334,  2.2969],\n",
       "         [ 0.9187,  0.7146, -0.0000, -1.2709,  2.1871,  0.7143,  0.0000,\n",
       "           1.5781, -1.3063,  1.3392,  0.0000,  2.2268],\n",
       "         [ 0.7400, -0.7442, -2.6102, -0.8779, -0.5755, -0.0963, -0.3161,\n",
       "          -0.5669,  0.0000,  0.2792,  1.2408,  0.3368],\n",
       "         [-0.8601, -0.0000, -0.4632, -1.5256,  0.0000, -0.9736,  1.1521,\n",
       "           0.4067, -0.1565,  0.5622,  0.0000,  3.2753],\n",
       "         [-0.3297, -1.4853, -0.3542,  0.3206,  2.7062,  1.5690,  0.2881,\n",
       "           0.6507,  0.4442,  1.6137, -2.1227,  1.2247],\n",
       "         [-1.5886,  2.6682, -1.9589, -0.0000,  0.0000,  1.1816,  2.0920,\n",
       "           2.1585, -1.5248, -0.4798,  0.5359,  2.2969],\n",
       "         [ 0.8427,  0.6230, -1.7769,  0.2384,  0.6843,  0.6268,  2.0736,\n",
       "           1.2419,  0.0678,  1.5014,  0.7810,  0.7619],\n",
       "         [ 0.0000,  0.9355, -0.4913, -0.2885,  0.7866,  0.5298,  0.5602,\n",
       "          -1.0924, -0.6791,  0.7429, -0.9087,  0.0914],\n",
       "         [ 0.8849, -0.3788, -2.8949,  0.6774,  0.6410, -0.6898, -1.4850,\n",
       "           0.7863, -1.3651,  1.5406, -0.8299,  0.9029],\n",
       "         [-0.9168, -1.8200, -1.1703, -1.0648,  1.7217,  1.6432, -1.0377,\n",
       "           1.3817, -0.8370,  1.6724,  0.1246,  0.7568],\n",
       "         [-1.5114,  0.8934, -0.2692,  1.3710,  0.0000,  2.2035,  1.0132,\n",
       "          -0.0912,  0.7801,  0.8235, -0.8323,  0.4751],\n",
       "         [-1.3722,  1.0784, -2.4777,  0.5422,  2.7289,  2.0161, -2.2390,\n",
       "          -0.1348,  0.0124,  1.1960,  0.3781,  1.3522],\n",
       "         [-0.3779,  0.6959, -2.3094,  0.7118,  2.7484,  1.9683, -2.2283,\n",
       "          -0.1376,  0.0148,  1.1959,  0.3786,  1.3522],\n",
       "         [-0.1625, -0.3475, -2.1088,  0.8415,  2.7656,  1.9197, -2.2176,\n",
       "          -0.1405,  0.0172,  1.1957,  0.3791,  1.3522],\n",
       "         [-0.9242, -1.0925, -1.8851,  0.9254,  2.7806,  1.8704, -0.0000,\n",
       "          -0.1435,  0.0196,  1.1956,  0.3796,  1.3522],\n",
       "         [-1.9626, -0.0000, -1.6486,  0.0000,  2.7932,  1.8204, -2.1962,\n",
       "          -0.1467,  0.0219,  1.1955,  0.3801,  1.3522]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
