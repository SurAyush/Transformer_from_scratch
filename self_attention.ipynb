{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will take 12 words's embedding as the input for attention\n",
    "# attention_score = softmax(q @ k.T / root(dk) + M)\n",
    "# q = query, k = key, dk = dimension of key, v: value\n",
    "# x_new = attention_score @ v\n",
    "# M: masking matrix\n",
    "\n",
    "c = 16   # dimension of word embedding\n",
    "b = 4    # batch size\n",
    "t = 12   # sequence length\n",
    "dk = 16  # dimension of key, query, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating random qkv\n",
    "q = torch.randn(b, t, dk)\n",
    "k = torch.randn(b, t, dk)\n",
    "v = torch.randn(b, t, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12, 15, 18])\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]])\n",
    "\n",
    "# dim=0 means column-wise sum (dim=0 means we have to reduce dimension 0(row) to find sum hence column-wise)\n",
    "print(torch.sum(matrix, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = torch.zeros(t, t)\n",
    "# for masked input, we set the value to -inf\n",
    "mask = torch.tril(torch.ones(t, t))\n",
    "# Convert to mask with -inf above the diagonal and 0 below\n",
    "lookahead_mask = mask.masked_fill(mask == 0, float('-inf'))  # -inf for the upper triangle\n",
    "lookahead_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4404, 0.5596, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7662, 0.1761, 0.0577, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3115, 0.4673, 0.1697, 0.0515, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1552, 0.2121, 0.2124, 0.3224, 0.0978, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2247, 0.1283, 0.3466, 0.1531, 0.1072, 0.0402, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1419, 0.1254, 0.1083, 0.1446, 0.3183, 0.1587, 0.0027, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0432, 0.0826, 0.0306, 0.1073, 0.0586, 0.0251, 0.5862, 0.0663,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2623, 0.0666, 0.0101, 0.2658, 0.0422, 0.0547, 0.0163, 0.2446,\n",
       "          0.0375, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0337, 0.1050, 0.0227, 0.1019, 0.0714, 0.0791, 0.3292, 0.1257,\n",
       "          0.0796, 0.0519, 0.0000, 0.0000],\n",
       "         [0.0896, 0.2609, 0.0300, 0.0461, 0.2117, 0.0223, 0.0891, 0.0627,\n",
       "          0.0423, 0.1092, 0.0361, 0.0000],\n",
       "         [0.0440, 0.2330, 0.0721, 0.0189, 0.0652, 0.0303, 0.0574, 0.0097,\n",
       "          0.1823, 0.0346, 0.0410, 0.2116]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3421, 0.6579, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5252, 0.2987, 0.1761, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3369, 0.0932, 0.4477, 0.1222, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1624, 0.5602, 0.0860, 0.1078, 0.0835, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0761, 0.0691, 0.1374, 0.3102, 0.0421, 0.3651, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1743, 0.0329, 0.0874, 0.1922, 0.0295, 0.2840, 0.1997, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0053, 0.0353, 0.0693, 0.2827, 0.2700, 0.1534, 0.1315, 0.0524,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0594, 0.0070, 0.4484, 0.0749, 0.0338, 0.0138, 0.0416, 0.0115,\n",
       "          0.3097, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0376, 0.0914, 0.0424, 0.0768, 0.1018, 0.0464, 0.0645, 0.5099,\n",
       "          0.0235, 0.0056, 0.0000, 0.0000],\n",
       "         [0.0495, 0.0323, 0.0921, 0.1396, 0.0604, 0.0938, 0.1689, 0.1431,\n",
       "          0.1471, 0.0149, 0.0585, 0.0000],\n",
       "         [0.0115, 0.0048, 0.0074, 0.0019, 0.0025, 0.0652, 0.0013, 0.0015,\n",
       "          0.0335, 0.5316, 0.0677, 0.2711]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.9036, 0.0964, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6477, 0.1700, 0.1823, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4848, 0.0718, 0.3981, 0.0454, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6834, 0.0845, 0.0968, 0.1243, 0.0109, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1520, 0.1818, 0.1174, 0.1897, 0.1966, 0.1626, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1140, 0.1509, 0.0744, 0.1548, 0.0225, 0.1784, 0.3050, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1338, 0.1968, 0.1980, 0.1726, 0.0864, 0.0349, 0.0670, 0.1105,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1255, 0.0780, 0.0667, 0.1316, 0.0805, 0.1575, 0.1659, 0.1391,\n",
       "          0.0552, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0420, 0.0414, 0.0314, 0.0170, 0.0616, 0.0514, 0.0656, 0.6209,\n",
       "          0.0166, 0.0521, 0.0000, 0.0000],\n",
       "         [0.0934, 0.0589, 0.1255, 0.0488, 0.1323, 0.0607, 0.0751, 0.1540,\n",
       "          0.0649, 0.0638, 0.1227, 0.0000],\n",
       "         [0.1347, 0.0602, 0.0492, 0.1938, 0.0311, 0.0500, 0.0611, 0.0423,\n",
       "          0.1381, 0.1177, 0.0888, 0.0329]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.9941, 0.0059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0764, 0.1713, 0.7523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1250, 0.3536, 0.1208, 0.4005, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0817, 0.1149, 0.6095, 0.1700, 0.0239, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0548, 0.2625, 0.1607, 0.3075, 0.0736, 0.1409, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2268, 0.1785, 0.1896, 0.1584, 0.0951, 0.0928, 0.0588, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1047, 0.0284, 0.2499, 0.5439, 0.0258, 0.0131, 0.0223, 0.0119,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3626, 0.0164, 0.0746, 0.1840, 0.0364, 0.0343, 0.0290, 0.1088,\n",
       "          0.1540, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4895, 0.0385, 0.0199, 0.0096, 0.0152, 0.1786, 0.0520, 0.0579,\n",
       "          0.0995, 0.0392, 0.0000, 0.0000],\n",
       "         [0.2415, 0.0314, 0.0497, 0.0215, 0.0671, 0.1912, 0.1157, 0.0335,\n",
       "          0.0675, 0.1371, 0.0438, 0.0000],\n",
       "         [0.0230, 0.1015, 0.1241, 0.1791, 0.0370, 0.0206, 0.0936, 0.0420,\n",
       "          0.0460, 0.3018, 0.0122, 0.0192]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transpose while keeping batch dimesion aside\n",
    "# dim: -1 means the last dimension for softmax (row-wise)\n",
    "attention_score = torch.softmax(q @ k.transpose(1, 2) / (dk ** 0.5) + lookahead_mask, dim=-1)\n",
    "attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8987e-01, -3.6762e-01, -4.0130e-01,  9.1304e-01, -1.2901e+00,\n",
       "           1.1330e+00, -9.4934e-01,  1.4501e+00,  2.0162e+00,  5.1108e-01,\n",
       "          -3.9820e-01, -1.1240e+00, -2.2025e-01,  1.3185e-01, -5.4909e-01,\n",
       "           1.4448e+00],\n",
       "         [-9.1582e-01, -7.1349e-01, -1.0895e+00,  9.7827e-01,  2.8869e-01,\n",
       "          -6.2276e-01, -7.3629e-01,  3.3440e-01,  1.1609e+00,  3.0013e-01,\n",
       "          -4.1741e-02, -5.9999e-01,  1.9257e-01, -7.9097e-01,  2.9768e-01,\n",
       "           7.9986e-01],\n",
       "         [-2.9783e-01, -4.5048e-01, -6.1011e-01,  8.4934e-01, -7.5715e-01,\n",
       "           4.7454e-01, -8.4245e-01,  1.1392e+00,  1.6208e+00,  3.9958e-01,\n",
       "          -2.5993e-01, -8.9800e-01, -5.2969e-02, -2.2669e-01, -3.5180e-01,\n",
       "           1.2150e+00],\n",
       "         [-4.6204e-01, -5.9353e-01, -1.0379e+00,  6.7314e-01,  1.2360e-01,\n",
       "          -6.3122e-01, -6.1380e-01,  6.8501e-01,  8.2316e-01,  2.1060e-01,\n",
       "          -9.5281e-03, -4.8987e-01,  2.3065e-01, -9.0163e-01, -4.5902e-03,\n",
       "           7.4851e-01],\n",
       "         [-1.2719e-01, -4.9628e-01, -9.6167e-01,  1.1943e-01, -4.9516e-01,\n",
       "          -8.7461e-02, -5.3319e-01,  1.4098e+00,  2.0997e-01,  4.7954e-01,\n",
       "          -6.8707e-02, -4.5470e-01,  7.7729e-02, -9.1117e-01, -2.8746e-01,\n",
       "           4.6734e-01],\n",
       "         [ 3.6199e-01, -3.0453e-01, -5.1068e-01,  2.1117e-02, -5.4009e-01,\n",
       "          -2.5772e-01, -5.2609e-01,  1.4308e+00,  3.2341e-01,  3.6207e-01,\n",
       "          -1.2032e-01, -3.2518e-01,  1.3857e-01, -7.2715e-01, -6.6841e-01,\n",
       "           6.3214e-01],\n",
       "         [ 1.3268e-02, -3.1938e-01,  9.0292e-03, -1.7028e-01, -2.3341e-01,\n",
       "          -7.0185e-01, -5.8332e-01,  8.9248e-01, -3.1544e-03,  8.0391e-01,\n",
       "          -2.5202e-01,  3.8368e-02,  1.0040e-01, -5.2861e-01, -3.8718e-01,\n",
       "           1.1162e-01],\n",
       "         [ 1.5987e-01, -2.8485e-01,  7.3460e-02, -2.3043e-01, -9.0365e-02,\n",
       "           6.3014e-01, -4.0728e-01,  1.7372e-01,  3.1070e-01,  7.5384e-01,\n",
       "          -5.8683e-01, -8.1074e-01, -8.4653e-01,  3.2105e-01, -4.5907e-01,\n",
       "           7.0660e-02],\n",
       "         [-2.9849e-01, -6.1866e-01, -1.0017e+00,  7.4871e-01, -5.2347e-01,\n",
       "           6.7214e-01, -6.1190e-01,  9.7099e-01,  4.7070e-01,  6.2241e-01,\n",
       "          -3.1503e-01, -7.1249e-01, -4.6624e-01,  4.7470e-02,  4.3617e-02,\n",
       "           5.7653e-01],\n",
       "         [-7.8802e-03, -3.8211e-01, -2.1400e-01, -7.9134e-02,  5.0889e-02,\n",
       "           2.4803e-01, -3.2678e-01,  1.7773e-01,  2.7619e-01,  6.2526e-01,\n",
       "          -3.8633e-01, -4.8353e-01, -5.5808e-01,  1.3877e-01, -1.1221e-01,\n",
       "           1.7002e-01],\n",
       "         [-3.4432e-01, -4.7537e-01, -1.7725e-01, -1.4065e-01,  2.7070e-01,\n",
       "          -5.3228e-01, -5.5044e-01,  3.0875e-01,  9.2390e-02,  6.1562e-01,\n",
       "          -8.8570e-02, -2.1033e-01, -1.2244e-01, -3.6897e-01, -1.8872e-01,\n",
       "           3.7549e-01],\n",
       "         [ 3.6900e-03, -3.2638e-01, -7.4830e-01, -2.9458e-01,  2.4356e-01,\n",
       "          -5.3872e-01, -5.1900e-01, -2.9627e-01, -1.1260e-01,  4.8378e-01,\n",
       "           2.6703e-01, -4.0091e-01, -1.3305e-01, -6.0731e-01,  1.2706e-02,\n",
       "           2.6371e-01]],\n",
       "\n",
       "        [[-2.3411e+00, -3.3561e-01,  9.1300e-01, -1.6952e+00, -8.2889e-01,\n",
       "           1.7007e-01,  6.0592e-01,  7.7890e-01,  5.4240e-02,  8.0224e-02,\n",
       "          -2.0033e-01,  6.2529e-02, -1.5623e+00, -1.1538e+00, -1.3983e-01,\n",
       "          -1.8129e+00],\n",
       "         [-1.3498e+00, -1.3309e+00,  8.0558e-02, -8.0250e-01, -3.6951e-01,\n",
       "          -3.1929e-01,  9.5319e-01,  1.4183e+00,  1.0291e-01, -3.0434e-01,\n",
       "          -6.4737e-01,  1.4607e-01, -6.8649e-01,  3.0431e-01, -5.5974e-01,\n",
       "          -1.5622e+00],\n",
       "         [-1.3982e+00, -6.4528e-01,  5.4468e-01, -8.9388e-01, -6.5040e-01,\n",
       "          -1.9339e-01,  5.5697e-01,  1.0084e+00,  1.0015e-01,  9.6813e-03,\n",
       "          -1.8025e-01,  1.3844e-01, -7.5251e-01, -1.6028e-01, -4.7762e-01,\n",
       "          -1.4275e+00],\n",
       "         [-7.4828e-01,  2.2462e-02,  5.3130e-01, -4.9967e-01, -7.8296e-01,\n",
       "          -2.9486e-01, -1.9774e-01,  5.5093e-01,  1.6631e-01,  2.8547e-01,\n",
       "           4.2832e-01, -5.1866e-02, -1.7959e-01,  1.5101e-01, -5.7241e-01,\n",
       "          -1.0138e+00],\n",
       "         [-8.1347e-01, -1.0719e+00, -9.9158e-02, -4.8550e-01, -3.1513e-01,\n",
       "          -4.0327e-01,  6.5178e-01,  1.0739e+00, -5.9632e-03, -3.0071e-01,\n",
       "          -2.8760e-01,  8.0737e-02, -1.9406e-01,  4.9786e-01, -4.9613e-01,\n",
       "          -1.2616e+00],\n",
       "         [-5.6725e-01, -5.7892e-02, -7.0054e-01, -5.7094e-01,  4.7421e-02,\n",
       "          -4.2935e-01, -2.9269e-01, -3.6343e-01,  3.8247e-01, -5.4176e-01,\n",
       "           4.8904e-01, -4.4884e-01, -2.2733e-01, -8.3465e-02,  8.8336e-02,\n",
       "          -3.8219e-01],\n",
       "         [-3.8973e-01, -2.9381e-01, -3.2628e-01, -6.4560e-01, -1.7758e-01,\n",
       "          -2.5189e-01, -1.9392e-01, -4.4975e-01,  7.6294e-01, -2.7368e-01,\n",
       "           4.6120e-01, -3.0356e-01, -3.5104e-01, -3.3532e-01,  1.7630e-01,\n",
       "          -3.9736e-01],\n",
       "         [ 1.2219e-01, -4.6968e-01, -3.5091e-01, -3.3528e-01, -9.9012e-02,\n",
       "          -3.5854e-01, -1.7188e-02, -4.2215e-01, -5.6666e-02, -3.4432e-01,\n",
       "           6.5043e-01, -1.0809e-01,  4.0356e-01,  2.1538e-02,  3.4441e-01,\n",
       "          -2.6176e-01],\n",
       "         [-1.4549e-01, -2.3740e-01,  1.4970e-01, -3.7690e-01, -3.1488e-01,\n",
       "          -4.9618e-01, -8.0445e-01, -1.5303e-01, -2.7926e-02,  9.1909e-01,\n",
       "           1.0340e+00,  2.0799e-01,  3.5338e-01, -1.1466e-01, -4.6961e-01,\n",
       "           7.1206e-01],\n",
       "         [-1.5201e-02, -1.4068e+00,  6.2534e-01, -1.1975e+00, -3.0951e-01,\n",
       "          -5.3494e-01, -4.5743e-01,  7.6292e-02, -1.4230e+00,  5.3416e-01,\n",
       "           5.3923e-01, -1.9784e-01,  9.4122e-01,  1.2106e+00,  9.9178e-01,\n",
       "           4.9684e-01],\n",
       "         [-3.7560e-02, -6.6626e-01, -4.5409e-02, -8.1845e-01, -1.7009e-01,\n",
       "          -4.1471e-01, -5.9860e-01, -3.7404e-01, -4.6676e-02,  4.4351e-01,\n",
       "           7.1875e-01, -1.9006e-01,  2.1440e-01,  1.4032e-01,  2.7458e-01,\n",
       "           4.8186e-01],\n",
       "         [-6.9087e-02, -3.7314e-01, -3.5351e-01, -5.8937e-01, -1.0211e+00,\n",
       "           3.5260e-01,  7.5425e-01, -1.1556e+00,  5.0296e-01,  8.3211e-01,\n",
       "           1.2065e-01, -2.9152e-01, -1.1197e+00, -2.9430e-01, -3.4366e-02,\n",
       "           7.4980e-01]],\n",
       "\n",
       "        [[-5.1867e-01, -1.3856e+00, -1.1380e+00, -9.3649e-01,  1.7309e-01,\n",
       "           5.3638e-01,  1.7714e-01, -1.7541e+00,  2.0838e+00,  2.7838e-01,\n",
       "          -9.5518e-02,  2.2436e-01,  1.4498e+00, -5.5421e-01,  7.3424e-01,\n",
       "          -1.3876e+00],\n",
       "         [-5.6894e-01, -1.2093e+00, -1.2091e+00, -8.2622e-01,  1.0052e-01,\n",
       "           5.8764e-01,  2.0855e-01, -1.7759e+00,  1.8238e+00,  2.9110e-01,\n",
       "          -1.0878e-01,  3.7175e-01,  1.3630e+00, -5.5044e-01,  6.5873e-01,\n",
       "          -1.2272e+00],\n",
       "         [-5.8715e-01, -8.3635e-01, -9.6974e-01, -4.5609e-01,  1.0970e-01,\n",
       "           9.0588e-01,  3.8812e-01, -1.5392e+00,  1.3182e+00,  2.0312e-01,\n",
       "          -2.7078e-01,  6.4782e-01,  1.0560e+00, -7.0033e-01,  8.9219e-01,\n",
       "          -7.7068e-01],\n",
       "         [-5.8045e-01, -6.7534e-01, -4.7084e-01, -1.8672e-01,  1.8829e-01,\n",
       "           1.1550e+00,  5.6747e-01, -1.1687e+00,  1.0776e+00,  9.7868e-05,\n",
       "          -4.0144e-01,  7.7910e-01,  7.6314e-01, -8.2367e-01,  1.3050e+00,\n",
       "          -4.8379e-01],\n",
       "         [-7.3540e-01, -9.0752e-01, -7.9186e-01, -5.7205e-01, -4.3218e-02,\n",
       "           6.3290e-01,  3.7476e-01, -1.5055e+00,  1.2885e+00,  2.9764e-02,\n",
       "          -9.0886e-02,  6.7524e-01,  9.6842e-01, -4.7952e-01,  7.8209e-01,\n",
       "          -9.0869e-01],\n",
       "         [-7.7131e-01,  2.2572e-01, -2.8114e-01, -2.3246e-01, -1.1649e-01,\n",
       "           4.6584e-01,  7.1649e-01, -9.5786e-01,  3.9924e-01, -3.3026e-01,\n",
       "          -2.2954e-01,  5.9200e-01,  4.5907e-01, -4.3435e-01,  3.1808e-01,\n",
       "          -2.7490e-01],\n",
       "         [-7.1719e-01,  1.1086e-01, -6.0155e-01,  2.1179e-02, -1.6581e-01,\n",
       "           6.1143e-01,  7.2128e-01, -3.3503e-01,  4.6055e-01, -9.6602e-02,\n",
       "          -3.5634e-01, -1.0990e-01,  3.8083e-02, -9.7205e-02,  2.7814e-01,\n",
       "          -1.1987e-01],\n",
       "         [-9.7608e-01, -6.3208e-02, -4.7065e-02, -4.1137e-02, -1.3070e-01,\n",
       "           5.5288e-01,  6.9985e-01, -6.9687e-01,  3.2570e-01, -9.2595e-02,\n",
       "          -2.9399e-01,  5.6424e-01,  3.4503e-01, -5.7013e-01,  4.7753e-01,\n",
       "           2.0944e-02],\n",
       "         [-7.8321e-01, -2.9597e-02, -1.6206e-01, -1.4809e-01,  4.4987e-03,\n",
       "           3.6491e-01,  6.7598e-01, -1.3599e-01,  6.7788e-01, -6.3927e-02,\n",
       "          -3.7585e-01, -2.2584e-01,  3.0757e-01, -3.0796e-01,  7.9593e-02,\n",
       "          -9.9961e-02],\n",
       "         [-1.4471e+00, -7.0966e-01,  8.0772e-01, -6.8923e-02,  1.2747e-02,\n",
       "          -4.0533e-01,  4.2270e-01,  9.1426e-01,  9.4776e-01,  3.9204e-01,\n",
       "          -5.8250e-01, -1.2398e+00,  9.3359e-01, -7.2926e-01, -5.0642e-01,\n",
       "           6.4282e-01],\n",
       "         [-7.0935e-01,  7.5616e-02,  1.2288e-01, -1.2899e-01,  1.3904e-01,\n",
       "           2.3849e-01,  4.6694e-01, -7.5746e-02,  5.7680e-01, -7.9886e-02,\n",
       "          -3.5091e-01, -1.5639e-01,  2.6598e-01, -6.6852e-01, -1.0142e-01,\n",
       "          -2.1045e-02],\n",
       "         [-7.1836e-01,  2.2457e-02, -1.9101e-01, -1.4898e-01, -5.7080e-02,\n",
       "           2.6696e-01,  3.4336e-01, -2.8763e-01,  3.7768e-01, -1.9982e-01,\n",
       "          -3.9377e-02,  4.0865e-01,  5.2004e-02, -1.8492e-01, -6.3260e-02,\n",
       "          -3.6317e-01]],\n",
       "\n",
       "        [[ 1.4652e+00,  5.2065e-01,  1.6651e-02,  5.9201e-01, -3.1049e-01,\n",
       "          -3.7434e-01,  7.7725e-01,  1.5290e+00,  8.5251e-01,  1.2803e-01,\n",
       "           4.6243e-01, -9.9557e-01, -2.7665e-01, -1.9390e+00,  2.0218e+00,\n",
       "           4.5100e-01],\n",
       "         [ 1.4522e+00,  5.2648e-01,  1.8337e-02,  5.9075e-01, -3.1258e-01,\n",
       "          -3.7207e-01,  7.8550e-01,  1.5228e+00,  8.5393e-01,  1.3136e-01,\n",
       "           4.5505e-01, -9.8547e-01, -2.7747e-01, -1.9235e+00,  2.0023e+00,\n",
       "           4.5037e-01],\n",
       "         [-6.0363e-01, -2.1451e-01,  1.9204e-01,  1.7781e-02,  2.2009e-01,\n",
       "          -1.0790e+00,  2.6346e+00,  1.2105e+00,  3.3897e-01, -1.6311e-01,\n",
       "          -7.9513e-01, -9.1026e-01,  1.3433e+00, -1.5653e+00,  2.4587e-01,\n",
       "          -2.4584e-01],\n",
       "         [ 2.5052e-01,  1.1029e+00, -6.4796e-01,  3.6062e-01, -2.6459e-01,\n",
       "           2.5265e-01,  1.4088e+00,  9.2670e-01, -8.3589e-02,  3.9129e-01,\n",
       "          -1.1078e-01,  2.7000e-01, -7.7746e-02,  3.7395e-01, -1.3539e-01,\n",
       "          -1.0445e-01],\n",
       "         [-2.5036e-01,  8.7670e-02, -1.8890e-01,  9.2260e-02,  1.2902e-01,\n",
       "          -6.6067e-01,  2.1752e+00,  1.1988e+00,  3.4790e-02, -1.0061e-01,\n",
       "          -5.5190e-01, -6.4715e-01,  1.0472e+00, -1.0880e+00,  2.5157e-01,\n",
       "          -2.7095e-01],\n",
       "         [ 9.7305e-02,  9.1618e-01, -5.6015e-01,  3.1322e-01, -2.5916e-01,\n",
       "           1.3175e-01,  1.3893e+00,  4.0383e-01,  2.4756e-02,  1.4875e-01,\n",
       "          -7.9431e-02, -6.2302e-02,  3.5810e-01,  2.3943e-01, -1.9313e-01,\n",
       "           3.7663e-02],\n",
       "         [ 1.8725e-01,  6.2481e-01, -2.7670e-01,  2.1226e-01, -2.4154e-01,\n",
       "          -6.4784e-02,  1.2808e+00,  6.8108e-01,  2.6395e-01, -3.3503e-04,\n",
       "          -1.3359e-01, -3.8264e-01,  3.8128e-01, -5.6481e-01,  1.2977e-01,\n",
       "           1.8804e-01],\n",
       "         [ 4.9250e-01,  7.7232e-01, -1.0090e+00,  2.5509e-01, -3.2655e-02,\n",
       "           2.5979e-01,  1.1276e+00,  1.0464e+00, -6.5604e-01,  1.4598e-01,\n",
       "           6.4979e-02, -2.0842e-02,  2.8799e-01,  1.1468e-01,  2.4391e-01,\n",
       "          -2.9768e-01],\n",
       "         [ 5.6296e-01,  7.1315e-01, -5.4946e-01,  3.1292e-01, -3.6448e-01,\n",
       "           2.2593e-01,  6.6740e-01,  8.6895e-01, -3.9470e-01, -1.5826e-01,\n",
       "           4.9088e-02, -5.2633e-01,  1.8013e-02, -4.0562e-01,  6.0573e-01,\n",
       "           1.3370e-01],\n",
       "         [ 5.6476e-01,  5.2390e-01, -2.3749e-01,  3.3064e-01, -3.4920e-01,\n",
       "          -1.1811e-01,  7.8800e-01,  3.6682e-01,  1.9755e-01, -9.6777e-02,\n",
       "           3.6734e-01, -9.2334e-01,  2.2879e-01, -7.8308e-01,  7.9374e-01,\n",
       "           3.7665e-01],\n",
       "         [ 7.7922e-02,  4.7067e-01, -3.7522e-01,  9.1688e-02, -4.5275e-01,\n",
       "          -9.0637e-02,  7.1136e-01,  1.0453e-01, -2.2268e-03, -1.1103e-01,\n",
       "           4.1345e-01, -8.1268e-01,  5.1054e-01, -5.7328e-01,  7.1370e-02,\n",
       "           3.4934e-01],\n",
       "         [-3.4891e-02,  7.8673e-01, -6.9327e-01,  2.8292e-02, -5.6613e-01,\n",
       "          -2.6052e-01,  9.3917e-01,  5.4807e-01, -4.5513e-01,  1.6042e-01,\n",
       "           3.6398e-01, -3.7463e-01,  3.9449e-01, -1.5334e-01, -5.0700e-01,\n",
       "          -7.4708e-02]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_temp = attention_score @ v\n",
    "x_temp       # after attention mechanism, the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_new = LayerNorm(x_temp + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-head attention\n",
    "# q, k, v are linearly projected h times with different weights\n",
    "# attention-values are concatenated\n",
    "# output is linearly projected again\n",
    "\n",
    "# casual-multi-head attention\n",
    "# carrying multi-head attention in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 12, 48])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(b, t, c)   # input\n",
    "lin_proj_1 = torch.nn.Linear (c, 3 * dk)  # linear projection (generating qkv together)\n",
    "qkv = lin_proj_1(X)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 200 artists>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdaklEQVR4nO3df3RX9X348VcQElDIB8MwgWMCqXWic7gOFaJup9KsjMNxckTX9riVOU47PZENsq0lO1XGTtdQt1VmD4rtHKznlNG6HWTOo7Ynq3h2CqhxnPnjyNTJCYoJto58MN9DwiGf7x+efmwULZ+QvC+f8Hicc88x93Nz8/KKfJ7nfu69qSgUCoUAAEhkXNYDAABnFvEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJjc96gPcbHByMgwcPxpQpU6KioiLrcQCAk1AoFOLIkSMxc+bMGDfuo89tnHbxcfDgwaivr896DABgGA4cOBDnn3/+R25z2sXHlClTIuLd4aurqzOeBgA4Gfl8Purr64vv4x/ltIuPn33UUl1dLT4AoMyczCUTLjgFAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQ1PusBgOzNXvNI1iOMuP3rl2Q9AvAhnPkAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJFVyfLzxxhvxe7/3ezFt2rSYNGlS/Oqv/mo888wzxdcLhULceeedMWPGjJg0aVI0NzfHyy+/PKJDAwDlq6T4+L//+7+4+uqrY8KECfHoo4/Giy++GH/3d38X5557bnGbu+66K+65557YtGlT7NmzJ84555xYtGhRHD16dMSHBwDKT0kPGfv6178e9fX1sXnz5uK6xsbG4j8XCoXYsGFDfOUrX4nrr78+IiK+853vRG1tbTz00EPx2c9+doTGBgDKVUlnPv7t3/4tLr/88rjpppvivPPOi0984hPx7W9/u/j6a6+9Ft3d3dHc3Fxcl8vlYv78+bFr166RmxoAKFslxcf//u//xn333RcXXnhhPP7443HbbbfFH//xH8c//dM/RUREd3d3RETU1tYO+b7a2tria+/X398f+Xx+yAIAjF0lfewyODgYl19+eXzta1+LiIhPfOIT8fzzz8emTZti+fLlwxqgvb091q1bN6zvBQDKT0lnPmbMmBGXXHLJkHUXX3xxdHV1RUREXV1dRET09PQM2aanp6f42vu1tbVFb29vcTlw4EApIwEAZaak+Lj66qtj3759Q9b9z//8T8yaNSsi3r34tK6uLjo6Ooqv5/P52LNnTzQ1NZ1wn1VVVVFdXT1kAQDGrpI+dlm9enVcddVV8bWvfS1+93d/N5566qn41re+Fd/61rciIqKioiJWrVoVX/3qV+PCCy+MxsbGuOOOO2LmzJmxdOnS0ZgfACgzJcXHFVdcEdu3b4+2trb4q7/6q2hsbIwNGzbEzTffXNzmS1/6UvT19cUXv/jFOHz4cFxzzTXx2GOPxcSJE0d8eACg/FQUCoVC1kP8vHw+H7lcLnp7e30EA4nMXvNI1iOMuP3rl2Q9ApxRSnn/9rtdAICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhqfNYDACNv9ppHsh4hc6Ueg/3rl4zSJMD7lXTm4y//8i+joqJiyDJnzpzi60ePHo2WlpaYNm1aTJ48OZYtWxY9PT0jPjQAUL5K/tjlV37lV+LNN98sLv/5n/9ZfG316tXx8MMPx4MPPhg7d+6MgwcPxg033DCiAwMA5a3kj13Gjx8fdXV1H1jf29sbDzzwQGzdujUWLlwYERGbN2+Oiy++OHbv3h0LFiw49WkBgLJX8pmPl19+OWbOnBkf+9jH4uabb46urq6IiOjs7Ixjx45Fc3Nzcds5c+ZEQ0ND7Nq160P319/fH/l8fsgCAIxdJZ35mD9/fmzZsiUuuuiiePPNN2PdunXxG7/xG/H8889Hd3d3VFZWxtSpU4d8T21tbXR3d3/oPtvb22PdunXDGh5gpHzYBaouRIWRV1J8LF68uPjPc+fOjfnz58esWbPi+9//fkyaNGlYA7S1tUVra2vx63w+H/X19cPaFwBw+jul53xMnTo1fvmXfzleeeWVqKuri4GBgTh8+PCQbXp6ek54jcjPVFVVRXV19ZAFABi7Tik+3nnnnXj11VdjxowZMW/evJgwYUJ0dHQUX9+3b190dXVFU1PTKQ8KAIwNJX3s8md/9mdx3XXXxaxZs+LgwYOxdu3aOOuss+Jzn/tc5HK5WLFiRbS2tkZNTU1UV1fHypUro6mpyZ0uAEBRSfHx+uuvx+c+97n46U9/GtOnT49rrrkmdu/eHdOnT4+IiLvvvjvGjRsXy5Yti/7+/li0aFHce++9ozI4AFCeKgqFQiHrIX5ePp+PXC4Xvb29rv+AYfJ49ZHjbhc4OaW8f/vFcgBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAfITZax7xuHoYYeIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS47MeADh5s9c8kvUIZ6z3H/v965dkNAmUP2c+AICkxAcAkJT4AACSEh8AQFLiAwBIyt0ucBpxNwtwJnDmAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkdUrxsX79+qioqIhVq1YV1x09ejRaWlpi2rRpMXny5Fi2bFn09PSc6pwAwBgx7Ph4+umn4/7774+5c+cOWb969ep4+OGH48EHH4ydO3fGwYMH44YbbjjlQQGAsWFY8fHOO+/EzTffHN/+9rfj3HPPLa7v7e2NBx54IL7xjW/EwoULY968ebF58+b48Y9/HLt37x6xoQGA8jWs+GhpaYklS5ZEc3PzkPWdnZ1x7NixIevnzJkTDQ0NsWvXrhPuq7+/P/L5/JAFABi7xpf6Ddu2bYtnn302nn766Q+81t3dHZWVlTF16tQh62tra6O7u/uE+2tvb49169aVOgYAUKZKOvNx4MCB+JM/+ZP47ne/GxMnThyRAdra2qK3t7e4HDhwYET2CwCcnkqKj87Ozjh06FD8+q//eowfPz7Gjx8fO3fujHvuuSfGjx8ftbW1MTAwEIcPHx7yfT09PVFXV3fCfVZVVUV1dfWQBQAYu0r62OVTn/pUPPfcc0PW3XLLLTFnzpz48pe/HPX19TFhwoTo6OiIZcuWRUTEvn37oqurK5qamkZuagCgbJUUH1OmTIlLL710yLpzzjknpk2bVly/YsWKaG1tjZqamqiuro6VK1dGU1NTLFiwYOSmBgDKVskXnP4id999d4wbNy6WLVsW/f39sWjRorj33ntH+scAAGWqolAoFLIe4ufl8/nI5XLR29vr+g/OOLPXPJL1CAzT/vVLsh4BMlXK+7ff7QIAJCU+AICkxAcAkJT4AACSEh8AQFIjfqstwJno/XcqufsFPpwzHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAjILZax75wCPXgXeJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhKfAAASY3PegA4E8xe80jWIwCcNpz5AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBSHq8OI8hj1Hm/k/0zsX/9klGeBE4fznwAAEmVFB/33XdfzJ07N6qrq6O6ujqampri0UcfLb5+9OjRaGlpiWnTpsXkyZNj2bJl0dPTM+JDAwDlq6T4OP/882P9+vXR2dkZzzzzTCxcuDCuv/76eOGFFyIiYvXq1fHwww/Hgw8+GDt37oyDBw/GDTfcMCqDAwDlqaJQKBROZQc1NTXxN3/zN3HjjTfG9OnTY+vWrXHjjTdGRMRLL70UF198cezatSsWLFhwUvvL5/ORy+Wit7c3qqurT2U0SM41HwyXaz4od6W8fw/7mo/jx4/Htm3boq+vL5qamqKzszOOHTsWzc3NxW3mzJkTDQ0NsWvXrg/dT39/f+Tz+SELADB2lRwfzz33XEyePDmqqqri1ltvje3bt8cll1wS3d3dUVlZGVOnTh2yfW1tbXR3d3/o/trb2yOXyxWX+vr6kv8lAIDyUXJ8XHTRRbF3797Ys2dP3HbbbbF8+fJ48cUXhz1AW1tb9Pb2FpcDBw4Me18AwOmv5Od8VFZWxsc//vGIiJg3b148/fTT8fd///fxmc98JgYGBuLw4cNDzn709PREXV3dh+6vqqoqqqqqSp8cAChLp/ycj8HBwejv74958+bFhAkToqOjo/javn37oqurK5qamk71xwAAY0RJZz7a2tpi8eLF0dDQEEeOHImtW7fGE088EY8//njkcrlYsWJFtLa2Rk1NTVRXV8fKlSujqanppO90AQDGvpLi49ChQ/H5z38+3nzzzcjlcjF37tx4/PHH47d+67ciIuLuu++OcePGxbJly6K/vz8WLVoU995776gMDgCUp1N+zsdI85wPypnnfDBcnvNBuUvynA8AgOEQHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACQlPgCApMQHAJDU+KwHACBi9ppHPvL1/euXJJoERp8zHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBIyuPV4RT8okdiA/BBznwAAEmJDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICmPVwcoA7/oUf771y9JNAmcOmc+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgqZLio729Pa644oqYMmVKnHfeebF06dLYt2/fkG2OHj0aLS0tMW3atJg8eXIsW7Ysenp6RnRoAKB8lRQfO3fujJaWlti9e3f88Ic/jGPHjsWnP/3p6OvrK26zevXqePjhh+PBBx+MnTt3xsGDB+OGG24Y8cEBgPJU0hNOH3vssSFfb9myJc4777zo7OyM3/zN34ze3t544IEHYuvWrbFw4cKIiNi8eXNcfPHFsXv37liwYMHITQ4AlKVTuuajt7c3IiJqamoiIqKzszOOHTsWzc3NxW3mzJkTDQ0NsWvXrhPuo7+/P/L5/JAFABi7hh0fg4ODsWrVqrj66qvj0ksvjYiI7u7uqKysjKlTpw7Ztra2Nrq7u0+4n/b29sjlcsWlvr5+uCMBAGVg2PHR0tISzz//fGzbtu2UBmhra4ve3t7icuDAgVPaHwBwehvWb7W9/fbb49///d/jySefjPPPP7+4vq6uLgYGBuLw4cNDzn709PREXV3dCfdVVVUVVVVVwxkDAChDJZ35KBQKcfvtt8f27dvjP/7jP6KxsXHI6/PmzYsJEyZER0dHcd2+ffuiq6srmpqaRmZiAKCslXTmo6WlJbZu3Ro7duyIKVOmFK/jyOVyMWnSpMjlcrFixYpobW2NmpqaqK6ujpUrV0ZTU5M7XQCAiIioKBQKhZPeuKLihOs3b94cf/AHfxAR7z5k7E//9E/jn//5n6O/vz8WLVoU995774d+7PJ++Xw+crlc9Pb2RnV19cmOBpmYveaRrEeAIfavX5L1CJyhSnn/LunMx8l0ysSJE2Pjxo2xcePGUnYNAJwh/G4XACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEmJDwAgKfEBACRV0uPV4Uzjd7cAjDxnPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAGENmr3nErwXgtCc+AICkxAcAkJT4AACSEh8AQFLiAwBIanzWA8DpwN0BAOk48wEAJCU+AICkxAcAkJT4AACScsEpwBj0YRdR71+/JPEk8EHOfAAASYkPACAp8QEAJCU+AICkxAcAkJS7XTgjeZw6Z6qf/dl31wtZcuYDAEhKfAAASYkPACAp8QEAJCU+AICk3O3CmORuFoDTV8lnPp588sm47rrrYubMmVFRUREPPfTQkNcLhULceeedMWPGjJg0aVI0NzfHyy+/PFLzAgBlruT46Ovri8suuyw2btx4wtfvuuuuuOeee2LTpk2xZ8+eOOecc2LRokVx9OjRUx4WACh/JX/ssnjx4li8ePEJXysUCrFhw4b4yle+Etdff31ERHznO9+J2traeOihh+Kzn/3sqU0LAJS9Eb3g9LXXXovu7u5obm4ursvlcjF//vzYtWvXSP4oAKBMjegFp93d3RERUVtbO2R9bW1t8bX36+/vj/7+/uLX+Xx+JEcCAE4zmd9q297eHrlcrrjU19dnPRIAMIpGND7q6uoiIqKnp2fI+p6enuJr79fW1ha9vb3F5cCBAyM5EgBwmhnR+GhsbIy6urro6Ogorsvn87Fnz55oamo64fdUVVVFdXX1kAUAGLtKvubjnXfeiVdeeaX49WuvvRZ79+6NmpqaaGhoiFWrVsVXv/rVuPDCC6OxsTHuuOOOmDlzZixdunQk5wYAylTJ8fHMM8/EtddeW/y6tbU1IiKWL18eW7ZsiS996UvR19cXX/ziF+Pw4cNxzTXXxGOPPRYTJ04cuakBgLJVUSgUClkP8fPy+Xzkcrno7e31EQwf4LHpkK3965dkPQKnqVLevzO/2wUAOLOIDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAkJT4AgKTEBwCQlPgAAJISHwBAUuIDAEhqfNYDwMmYveaRrEcAYIQ48wEAJCU+AICkxAcAkJT4AACSEh8AQFLudiEJd6vA2PCz/5f3r1+S8SSUM2c+AICkxAcAkJT4AACSEh8AQFLiAwBIyt0uAJTsdL2DzV045cGZDwAgKfEBACQlPgCApMQHAJCU+AAAkhIfAEBS4gMASEp8AABJiQ8AICnxAQAk5fHqnJTT9VHKAJQfZz4AgKTEBwCQlPgAAJISHwBAUuIDAEjK3S6nyF0gAKeP0fo7ef/6JaOy3zPVqJ352LhxY8yePTsmTpwY8+fPj6eeemq0fhQAUEZGJT6+973vRWtra6xduzaeffbZuOyyy2LRokVx6NCh0fhxAEAZGZX4+MY3vhFf+MIX4pZbbolLLrkkNm3aFGeffXb84z/+42j8OACgjIz4NR8DAwPR2dkZbW1txXXjxo2L5ubm2LVr1we27+/vj/7+/uLXvb29ERGRz+dHerRRMdj//7IeAYBRVi7vSVn62TEqFAq/cNsRj4+f/OQncfz48aitrR2yvra2Nl566aUPbN/e3h7r1q37wPr6+vqRHg0AhiW3IesJyseRI0cil8t95DaZ3+3S1tYWra2txa8HBwfj7bffjmnTpkVFRUWGk52afD4f9fX1ceDAgaiurs56nMw4Du9xLN7lOLzHsXiPY/Gucj4OhUIhjhw5EjNnzvyF2454fPzSL/1SnHXWWdHT0zNkfU9PT9TV1X1g+6qqqqiqqhqyburUqSM9Vmaqq6vL7g/QaHAc3uNYvMtxeI9j8R7H4l3lehx+0RmPnxnxC04rKytj3rx50dHRUVw3ODgYHR0d0dTUNNI/DgAoM6PysUtra2ssX748Lr/88rjyyitjw4YN0dfXF7fccsto/DgAoIyMSnx85jOfibfeeivuvPPO6O7ujl/7tV+Lxx577AMXoY5lVVVVsXbt2g98pHSmcRze41i8y3F4j2PxHsfiXWfKcagonMw9MQAAI8QvlgMAkhIfAEBS4gMASEp8AABJiY8Efud3ficaGhpi4sSJMWPGjPj93//9OHjwYNZjJbV///5YsWJFNDY2xqRJk+KCCy6ItWvXxsDAQNajZeKv//qv46qrroqzzz57TD1U72Rs3LgxZs+eHRMnToz58+fHU089lfVIyT355JNx3XXXxcyZM6OioiIeeuihrEfKRHt7e1xxxRUxZcqUOO+882Lp0qWxb9++rMfKxH333Rdz584tPlysqakpHn300azHGjXiI4Frr702vv/978e+ffviX//1X+PVV1+NG2+8MeuxknrppZdicHAw7r///njhhRfi7rvvjk2bNsVf/MVfZD1aJgYGBuKmm26K2267LetRkvre974Xra2tsXbt2nj22Wfjsssui0WLFsWhQ4eyHi2pvr6+uOyyy2Ljxo1Zj5KpnTt3RktLS+zevTt++MMfxrFjx+LTn/509PX1ZT1acueff36sX78+Ojs745lnnomFCxfG9ddfHy+88ELWo42OAsnt2LGjUFFRURgYGMh6lEzdddddhcbGxqzHyNTmzZsLuVwu6zGSufLKKwstLS3Fr48fP16YOXNmob29PcOpshURhe3bt2c9xmnh0KFDhYgo7Ny5M+tRTgvnnntu4R/+4R+yHmNUOPOR2Ntvvx3f/e5346qrrooJEyZkPU6ment7o6amJusxSGRgYCA6Ozujubm5uG7cuHHR3Nwcu3btynAyThe9vb0REWf83wvHjx+Pbdu2RV9f35j9tSTiI5Evf/nLcc4558S0adOiq6srduzYkfVImXrllVfim9/8ZvzRH/1R1qOQyE9+8pM4fvz4B550XFtbG93d3RlNxelicHAwVq1aFVdffXVceumlWY+Tieeeey4mT54cVVVVceutt8b27dvjkksuyXqsUSE+hmnNmjVRUVHxkctLL71U3P7P//zP47/+67/iBz/4QZx11lnx+c9/Pgpj4OGypR6HiIg33ngjfvu3fztuuumm+MIXvpDR5CNvOMcCeFdLS0s8//zzsW3btqxHycxFF10Ue/fujT179sRtt90Wy5cvjxdffDHrsUaFx6sP01tvvRU//elPP3Kbj33sY1FZWfmB9a+//nrU19fHj3/847I/pVbqcTh48GB88pOfjAULFsSWLVti3Lix07/D+TOxZcuWWLVqVRw+fHiUp8vewMBAnH322fEv//IvsXTp0uL65cuXx+HDh8/Ys4EVFRWxffv2IcfkTHP77bfHjh074sknn4zGxsasxzltNDc3xwUXXBD3339/1qOMuFH5xXJngunTp8f06dOH9b2Dg4MREdHf3z+SI2WilOPwxhtvxLXXXhvz5s2LzZs3j6nwiDi1PxNngsrKypg3b150dHQU32gHBwejo6Mjbr/99myHIxOFQiFWrlwZ27dvjyeeeEJ4vM/g4OCYeJ84EfExyvbs2RNPP/10XHPNNXHuuefGq6++GnfccUdccMEFZX/WoxRvvPFGfPKTn4xZs2bF3/7t38Zbb71VfK2uri7DybLR1dUVb7/9dnR1dcXx48dj7969ERHx8Y9/PCZPnpztcKOotbU1li9fHpdffnlceeWVsWHDhujr64tbbrkl69GSeuedd+KVV14pfv3aa6/F3r17o6amJhoaGjKcLK2WlpbYunVr7NixI6ZMmVK89ieXy8WkSZMyni6ttra2WLx4cTQ0NMSRI0di69at8cQTT8Tjjz+e9WijI9ubbca+//7v/y5ce+21hZqamkJVVVVh9uzZhVtvvbXw+uuvZz1aUps3by5ExAmXM9Hy5ctPeCx+9KMfZT3aqPvmN79ZaGhoKFRWVhauvPLKwu7du7MeKbkf/ehHJ/zvv3z58qxHS+rD/k7YvHlz1qMl94d/+IeFWbNmFSorKwvTp08vfOpTnyr84Ac/yHqsUeOaDwAgqbH1oTsAcNoTHwBAUuIDAEhKfAAASYkPACAp8QEAJCU+AICkxAcAkJT4AACSEh8AQFLiAwBISnwAAEn9f4m8g4KwKL1KAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "y_val = torch.histc(qkv, bins=200, min=-3, max=3)\n",
    "x_val = np.arange(-1, 1, 0.01) * 3\n",
    "plt.bar(x_val, y_val, align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# near normal distribution\n",
    "# split the qkv into n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 12, 4, 12])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = qkv.shape[-1]\n",
    "qkv = qkv.reshape(b, t, n_heads, d // n_heads)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to do pooling of batch_dim and head_dim and perform cal with T * C\n",
    "qkv = qkv.permute(0,2,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v = torch.chunk(qkv, 3, dim=-1)    # simple split along the last dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 12, 4])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch: 4  heads: 4 seq_len: 12 head_size:4\n",
    "# dk = n_heads * head_size\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (B,H,T,D) @ (B,H,D,T) = (B,H,T,T)\n",
    "attention_score = torch.softmax(q @ k.transpose(-2, -1) / (dk ** 0.5), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0929, 0.0831, 0.0714,  ..., 0.0674, 0.1031, 0.1233],\n",
       "          [0.0862, 0.0812, 0.0784,  ..., 0.0856, 0.0853, 0.0925],\n",
       "          [0.0672, 0.0649, 0.0872,  ..., 0.0943, 0.0883, 0.0864],\n",
       "          ...,\n",
       "          [0.0929, 0.0919, 0.0800,  ..., 0.0851, 0.0753, 0.0772],\n",
       "          [0.0635, 0.0749, 0.0924,  ..., 0.0950, 0.0831, 0.0767],\n",
       "          [0.0764, 0.0848, 0.0873,  ..., 0.0887, 0.0798, 0.0767]],\n",
       "\n",
       "         [[0.0934, 0.0899, 0.0793,  ..., 0.0829, 0.0694, 0.0787],\n",
       "          [0.0755, 0.0842, 0.0761,  ..., 0.0841, 0.0707, 0.0724],\n",
       "          [0.0502, 0.0663, 0.0898,  ..., 0.0856, 0.0931, 0.0754],\n",
       "          ...,\n",
       "          [0.0936, 0.0875, 0.0881,  ..., 0.0833, 0.0801, 0.0862],\n",
       "          [0.0376, 0.0694, 0.0935,  ..., 0.0925, 0.0909, 0.0625],\n",
       "          [0.0810, 0.0867, 0.0868,  ..., 0.0851, 0.0864, 0.0817]],\n",
       "\n",
       "         [[0.0660, 0.0871, 0.0852,  ..., 0.0829, 0.0724, 0.0987],\n",
       "          [0.0815, 0.0840, 0.0832,  ..., 0.0788, 0.0806, 0.0989],\n",
       "          [0.1307, 0.0857, 0.0753,  ..., 0.0669, 0.1016, 0.0706],\n",
       "          ...,\n",
       "          [0.0883, 0.0927, 0.0799,  ..., 0.0780, 0.0821, 0.0787],\n",
       "          [0.1122, 0.0835, 0.0755,  ..., 0.0751, 0.0922, 0.0736],\n",
       "          [0.0792, 0.0893, 0.0938,  ..., 0.0806, 0.0898, 0.0831]],\n",
       "\n",
       "         [[0.0833, 0.0833, 0.0860,  ..., 0.0824, 0.0854, 0.0812],\n",
       "          [0.0823, 0.0758, 0.0967,  ..., 0.0861, 0.0781, 0.0720],\n",
       "          [0.0693, 0.0781, 0.0896,  ..., 0.0871, 0.0737, 0.0962],\n",
       "          ...,\n",
       "          [0.0746, 0.0744, 0.1089,  ..., 0.0863, 0.0891, 0.0782],\n",
       "          [0.1233, 0.0996, 0.0390,  ..., 0.0791, 0.0718, 0.0848],\n",
       "          [0.0942, 0.0691, 0.0796,  ..., 0.0961, 0.0583, 0.0657]]],\n",
       "\n",
       "\n",
       "        [[[0.0674, 0.0813, 0.0684,  ..., 0.0932, 0.0966, 0.0820],\n",
       "          [0.0808, 0.0823, 0.0890,  ..., 0.0806, 0.0812, 0.0827],\n",
       "          [0.1004, 0.0813, 0.0946,  ..., 0.0772, 0.0709, 0.0862],\n",
       "          ...,\n",
       "          [0.0988, 0.0828, 0.1012,  ..., 0.0743, 0.0699, 0.0853],\n",
       "          [0.0714, 0.1006, 0.0696,  ..., 0.0848, 0.1067, 0.0745],\n",
       "          [0.0643, 0.1070, 0.0687,  ..., 0.0888, 0.1121, 0.0787]],\n",
       "\n",
       "         [[0.0625, 0.0969, 0.0783,  ..., 0.1034, 0.0953, 0.0943],\n",
       "          [0.0954, 0.0688, 0.0807,  ..., 0.0691, 0.0830, 0.0648],\n",
       "          [0.0754, 0.1103, 0.0877,  ..., 0.0856, 0.0783, 0.0915],\n",
       "          ...,\n",
       "          [0.0877, 0.0800, 0.0818,  ..., 0.0753, 0.0853, 0.0785],\n",
       "          [0.0890, 0.0633, 0.0777,  ..., 0.0716, 0.0882, 0.0503],\n",
       "          [0.0874, 0.0687, 0.0795,  ..., 0.0836, 0.0832, 0.0901]],\n",
       "\n",
       "         [[0.0889, 0.0443, 0.0474,  ..., 0.0783, 0.0811, 0.0705],\n",
       "          [0.0708, 0.1521, 0.1337,  ..., 0.0822, 0.0761, 0.0756],\n",
       "          [0.0914, 0.0672, 0.0817,  ..., 0.0737, 0.0750, 0.0861],\n",
       "          ...,\n",
       "          [0.0845, 0.0943, 0.1342,  ..., 0.0673, 0.0674, 0.1122],\n",
       "          [0.1029, 0.0715, 0.0912,  ..., 0.0591, 0.0700, 0.0977],\n",
       "          [0.0884, 0.0844, 0.0809,  ..., 0.0762, 0.0842, 0.0875]],\n",
       "\n",
       "         [[0.1047, 0.0768, 0.0874,  ..., 0.0744, 0.0659, 0.1225],\n",
       "          [0.0621, 0.0787, 0.0668,  ..., 0.0847, 0.1144, 0.0456],\n",
       "          [0.0826, 0.0770, 0.0869,  ..., 0.0896, 0.0786, 0.0710],\n",
       "          ...,\n",
       "          [0.0626, 0.1003, 0.1123,  ..., 0.1025, 0.0724, 0.0610],\n",
       "          [0.0672, 0.0844, 0.0594,  ..., 0.0828, 0.1190, 0.0637],\n",
       "          [0.1511, 0.0476, 0.0740,  ..., 0.0605, 0.0521, 0.1385]]],\n",
       "\n",
       "\n",
       "        [[[0.0930, 0.0759, 0.0912,  ..., 0.0853, 0.0797, 0.0856],\n",
       "          [0.0870, 0.0895, 0.0895,  ..., 0.0844, 0.0820, 0.0815],\n",
       "          [0.0856, 0.0757, 0.0847,  ..., 0.0914, 0.0841, 0.0783],\n",
       "          ...,\n",
       "          [0.0590, 0.1247, 0.0645,  ..., 0.0742, 0.0941, 0.0708],\n",
       "          [0.0858, 0.0852, 0.0824,  ..., 0.0794, 0.0858, 0.0797],\n",
       "          [0.0753, 0.0899, 0.0871,  ..., 0.0961, 0.0790, 0.0849]],\n",
       "\n",
       "         [[0.0870, 0.0782, 0.1144,  ..., 0.0924, 0.0625, 0.0855],\n",
       "          [0.1249, 0.0777, 0.1047,  ..., 0.0639, 0.0964, 0.0655],\n",
       "          [0.0642, 0.1054, 0.0541,  ..., 0.0659, 0.1112, 0.0781],\n",
       "          ...,\n",
       "          [0.0843, 0.0777, 0.1011,  ..., 0.0867, 0.0779, 0.0856],\n",
       "          [0.1417, 0.0799, 0.1003,  ..., 0.0515, 0.1168, 0.0566],\n",
       "          [0.0644, 0.0903, 0.0627,  ..., 0.0834, 0.0882, 0.0878]],\n",
       "\n",
       "         [[0.0792, 0.0941, 0.0812,  ..., 0.0957, 0.1003, 0.0923],\n",
       "          [0.0880, 0.1078, 0.0852,  ..., 0.0988, 0.0861, 0.0998],\n",
       "          [0.0821, 0.0745, 0.0872,  ..., 0.0807, 0.0726, 0.0791],\n",
       "          ...,\n",
       "          [0.0727, 0.0543, 0.0883,  ..., 0.0742, 0.0737, 0.0696],\n",
       "          [0.0888, 0.1266, 0.0779,  ..., 0.0992, 0.0927, 0.1013],\n",
       "          [0.0820, 0.0652, 0.0794,  ..., 0.0659, 0.0746, 0.0693]],\n",
       "\n",
       "         [[0.0883, 0.0969, 0.0766,  ..., 0.0660, 0.0932, 0.0654],\n",
       "          [0.0791, 0.0814, 0.0980,  ..., 0.0822, 0.0875, 0.0850],\n",
       "          [0.0976, 0.0887, 0.0679,  ..., 0.0836, 0.0893, 0.0689],\n",
       "          ...,\n",
       "          [0.0769, 0.0638, 0.0768,  ..., 0.0918, 0.0693, 0.0798],\n",
       "          [0.0877, 0.0901, 0.0835,  ..., 0.0799, 0.0905, 0.0786],\n",
       "          [0.0840, 0.0940, 0.0916,  ..., 0.0801, 0.0913, 0.0891]]],\n",
       "\n",
       "\n",
       "        [[[0.0947, 0.0824, 0.0972,  ..., 0.0838, 0.0831, 0.0743],\n",
       "          [0.0744, 0.0781, 0.0845,  ..., 0.0898, 0.0856, 0.0927],\n",
       "          [0.0566, 0.0941, 0.0910,  ..., 0.0688, 0.0930, 0.1006],\n",
       "          ...,\n",
       "          [0.0759, 0.1070, 0.0773,  ..., 0.0592, 0.0844, 0.0833],\n",
       "          [0.0831, 0.0866, 0.0942,  ..., 0.0803, 0.0861, 0.0778],\n",
       "          [0.0786, 0.0836, 0.0882,  ..., 0.0825, 0.0856, 0.0880]],\n",
       "\n",
       "         [[0.0806, 0.0940, 0.0841,  ..., 0.0830, 0.0815, 0.0791],\n",
       "          [0.0844, 0.0700, 0.0623,  ..., 0.0732, 0.0804, 0.0933],\n",
       "          [0.0775, 0.1009, 0.0981,  ..., 0.0917, 0.0824, 0.0712],\n",
       "          ...,\n",
       "          [0.0936, 0.0715, 0.0697,  ..., 0.0635, 0.0828, 0.1092],\n",
       "          [0.0869, 0.0703, 0.0707,  ..., 0.0612, 0.0830, 0.1077],\n",
       "          [0.0900, 0.0637, 0.0644,  ..., 0.0611, 0.0827, 0.1119]],\n",
       "\n",
       "         [[0.0799, 0.0800, 0.0798,  ..., 0.0913, 0.0880, 0.0807],\n",
       "          [0.0897, 0.0796, 0.0797,  ..., 0.0855, 0.0788, 0.0667],\n",
       "          [0.0867, 0.0630, 0.0852,  ..., 0.0907, 0.0779, 0.0788],\n",
       "          ...,\n",
       "          [0.0883, 0.0926, 0.0893,  ..., 0.0682, 0.0657, 0.0932],\n",
       "          [0.0792, 0.0988, 0.0796,  ..., 0.0930, 0.0811, 0.0899],\n",
       "          [0.0828, 0.0886, 0.0841,  ..., 0.0876, 0.0754, 0.0920]],\n",
       "\n",
       "         [[0.0853, 0.0849, 0.0822,  ..., 0.0833, 0.0854, 0.0807],\n",
       "          [0.0959, 0.0762, 0.0850,  ..., 0.0842, 0.0933, 0.0802],\n",
       "          [0.0700, 0.0701, 0.0864,  ..., 0.0868, 0.0901, 0.0966],\n",
       "          ...,\n",
       "          [0.0669, 0.0761, 0.0812,  ..., 0.0876, 0.0852, 0.0938],\n",
       "          [0.1068, 0.0851, 0.0862,  ..., 0.0774, 0.0829, 0.0727],\n",
       "          [0.0795, 0.0964, 0.1085,  ..., 0.0596, 0.0656, 0.0800]]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (B,H,T,T) @ (B,H,T,D) = (B,H,T,D)\n",
    "x_temp = attention_score @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSA(nn.Module):\n",
    "    # constructor\n",
    "    def __init__(self, n_heads, d_model, input_dim):  \n",
    "        super().__init__()    \n",
    "        assert d_model % n_heads == 0 , \"Invalid head_size for the given d_model\"\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_size = d_model // n_heads\n",
    "        self.input_dim = input_dim\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3 * d_model)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, X, mask = None):\n",
    "        B, T, C = X.shape\n",
    "        assert C == self.input_dim, \"Input dimension does not match the model input dimension\"\n",
    "        qkv = self.qkv_proj(X)                                    # (B,T,3*D)\n",
    "        qkv = qkv.reshape(B, T, self.n_heads, 3 * self.d_model // self.n_heads)\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "        if mask is None:\n",
    "            attention_score = torch.softmax(q @ k.transpose(-2, -1) / (self.head_size ** 0.5), dim=-1)\n",
    "        else:\n",
    "            attention_score = torch.softmax(q @ k.transpose(-2, -1) / (self.head_size ** 0.5) + mask, dim=-1)\n",
    "        res = attention_score @ v                                       # (B,H,T,head_size)\n",
    "        res = res.permute(0,2,1,3).reshape(B, T, self.d_model)   \n",
    "        res = self.linear(res)\n",
    "\n",
    "        return res               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1187, -0.0802,  0.2717,  ...,  0.1765, -0.3697, -0.1359],\n",
       "         [-0.1015, -0.1298,  0.3504,  ...,  0.2256, -0.3271, -0.2016],\n",
       "         [-0.1035, -0.0761,  0.2271,  ...,  0.2417, -0.4300, -0.1929],\n",
       "         [-0.0803, -0.0485,  0.2259,  ...,  0.2825, -0.3690, -0.0985],\n",
       "         [-0.0925, -0.0296,  0.2512,  ...,  0.1752, -0.3954, -0.1690]],\n",
       "\n",
       "        [[-0.0430, -0.0092, -0.1581,  ..., -0.0421,  0.1594, -0.2107],\n",
       "         [ 0.0212, -0.0127, -0.2500,  ...,  0.0011,  0.1214, -0.3348],\n",
       "         [-0.0139,  0.0245, -0.2587,  ..., -0.1047,  0.1383, -0.3125],\n",
       "         [ 0.0167, -0.0491, -0.2390,  ...,  0.0016,  0.1532, -0.1549],\n",
       "         [-0.0440, -0.0100, -0.2521,  ..., -0.0383,  0.1960, -0.2637]],\n",
       "\n",
       "        [[ 0.0695,  0.0823,  0.0430,  ...,  0.0030,  0.0080, -0.1310],\n",
       "         [-0.0080,  0.0683, -0.0282,  ..., -0.0559,  0.0234, -0.0781],\n",
       "         [ 0.0118,  0.0380, -0.0590,  ..., -0.0470,  0.0515, -0.0642],\n",
       "         [ 0.0229,  0.0035, -0.0518,  ..., -0.0286,  0.0356, -0.1156],\n",
       "         [ 0.0477,  0.0388, -0.0289,  ...,  0.0006,  0.1058, -0.0427]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.1162,  0.0168,  0.0849,  ..., -0.0364, -0.1802, -0.0156],\n",
       "         [-0.0091,  0.0194,  0.0736,  ..., -0.1016, -0.0600,  0.0144],\n",
       "         [ 0.1410,  0.0090,  0.0858,  ..., -0.1455, -0.1743,  0.0052],\n",
       "         [ 0.0583, -0.0750,  0.0189,  ..., -0.1378, -0.1130,  0.0996],\n",
       "         [ 0.1134,  0.0108,  0.0508,  ..., -0.1773, -0.1033,  0.0882]],\n",
       "\n",
       "        [[ 0.0497,  0.0175, -0.3789,  ...,  0.0498,  0.2974, -0.2464],\n",
       "         [-0.0073,  0.1050, -0.3256,  ..., -0.0141,  0.2536, -0.0975],\n",
       "         [ 0.1206,  0.0181, -0.1904,  ...,  0.0348,  0.1069, -0.0016],\n",
       "         [ 0.1352, -0.0356, -0.2872,  ...,  0.0786,  0.2299, -0.1914],\n",
       "         [ 0.0494,  0.0431, -0.3594,  ...,  0.0297,  0.2277, -0.1861]],\n",
       "\n",
       "        [[-0.2044,  0.1394,  0.0324,  ..., -0.0675, -0.1394,  0.2464],\n",
       "         [-0.1576,  0.1522,  0.1076,  ..., -0.0308, -0.1427,  0.3599],\n",
       "         [-0.1520,  0.1745,  0.0569,  ..., -0.0507, -0.2009,  0.3217],\n",
       "         [-0.2034,  0.1473,  0.0251,  ..., -0.1296, -0.1930,  0.2791],\n",
       "         [-0.1918,  0.1755,  0.1233,  ..., -0.0370, -0.2373,  0.2756]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 1024\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "x = torch.randn( (batch_size, sequence_length, input_dim) )\n",
    "x.shape\n",
    "model = MultiHeadSA(num_heads, d_model, input_dim)\n",
    "out = model.forward(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
